{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Memorix AI","text":"![Memorix AI Logo](../img/MEMORIX-AI.png)    **AI Memory Management Framework**  [![PyPI version](https://badge.fury.io/py/memorix.svg)](https://badge.fury.io/py/memorix) [![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) [![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)"},{"location":"#what-is-memorix-ai","title":"What is Memorix AI?","text":"<p>Memorix AI is a powerful Python framework designed to manage and enhance AI memory systems. It provides a unified interface for working with various vector stores, embedding models, and memory management strategies.</p>"},{"location":"#key-features","title":"Key Features","text":"#### \ud83e\udde0 Intelligent Memory Management Store, retrieve, and manage AI conversation history and knowledge with advanced semantic search capabilities.     #### \ud83d\udd0d Multi-Vector Store Support Work with Chroma, Pinecone, Weaviate, and more vector databases seamlessly through a unified API.     #### \ud83c\udfaf Flexible Embedding Models Support for OpenAI, Hugging Face, and custom embedding models to match your specific needs.     #### \u26a1 High Performance Optimized for speed and efficiency with batch operations and intelligent caching.     #### \ud83d\udd27 Easy Integration Simple API that works with any AI framework - from chatbots to knowledge bases.     #### \ud83d\udcca Rich Metadata Store and query contextual information alongside embeddings for powerful filtering and organization."},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from memorix import MemoryAPI\n\n# Initialize memory system\nmemory = MemoryAPI(\n    vector_store=\"chroma\",\n    embedder=\"openai\",\n    collection_name=\"my_chatbot\"\n)\n\n# Store a conversation\nmemory.store(\n    text=\"User asked about Python programming\",\n    metadata={\"user_id\": \"123\", \"timestamp\": \"2024-01-01\"}\n)\n\n# Retrieve relevant memories\nresults = memory.retrieve(\n    query=\"Python programming help\",\n    top_k=5\n)\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install memorix\n</code></pre>"},{"location":"#documentation-sections","title":"Documentation Sections","text":"#### \ud83d\udcda Installation Get started with Memorix AI in minutes with our comprehensive installation guide.  Get Started   #### \ud83d\ude80 Quick Start Build your first memory system with step-by-step tutorials and examples.  Start Building   #### \ud83d\udcd6 User Guide Learn the core concepts and advanced features of Memorix AI.  Learn More   #### \ud83d\udd27 API Reference Complete API documentation with examples and best practices.  View API   #### \ud83d\udca1 Examples Real-world usage examples and implementation patterns.  See Examples   #### \ud83e\udd1d Contributing Join our community and help improve Memorix AI.  Contribute"},{"location":"#community","title":"Community","text":"- \ud83d\udcd6 [Documentation](https://memorix-ai.github.io/memorix-docs/) - \ud83d\udc1b [Report Issues](https://github.com/memorix-ai/memorix-sdk/issues) - \ud83d\udcac [Discussions](https://github.com/memorix-ai/memorix-sdk/discussions) - \ud83e\udd1d [Contributing](contributing.md)  ### License  Memorix AI is licensed under the Apache 2.0 License. See the [LICENSE](../LICENSE) file for details.  **Built with \u2764\ufe0f by the Memorix AI Team**"},{"location":"architecture/","title":"Architecture","text":"<p>This page describes the architecture of Memorix AI.</p>"},{"location":"architecture/#system-overview","title":"System Overview","text":"<p>This documentation is under development.</p>"},{"location":"architecture/#components","title":"Components","text":"<ul> <li>Memory API</li> <li>Vector Store Layer</li> <li>Embedding Layer</li> <li>Configuration Management </li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to Memorix AI will be documented in this file.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Initial documentation setup</li> <li>CI/CD pipeline</li> <li>GitHub Actions workflows</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>None</li> </ul>"},{"location":"changelog/#deprecated","title":"Deprecated","text":"<ul> <li>None</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>None</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>None</li> </ul>"},{"location":"changelog/#security","title":"Security","text":"<ul> <li>None </li> </ul>"},{"location":"contributing/","title":"Contributing to Memorix AI","text":"<p>Thank you for your interest in contributing to Memorix AI! This guide will help you get started.</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":"<p>This documentation is under development.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Fork the repository</li> <li>Clone your fork</li> <li>Install dependencies</li> <li>Make your changes</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>We are committed to providing a welcoming and inspiring community for all. </p>"},{"location":"install/","title":"Installation","text":"<p>This guide will help you install Memorix AI and get it running on your system.</p>"},{"location":"install/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.8 or higher</li> <li>Operating System: Windows, macOS, or Linux</li> <li>Memory: At least 4GB RAM (8GB recommended for large datasets)</li> <li>Storage: 1GB free space for the package and dependencies</li> </ul>"},{"location":"install/#installation-methods","title":"Installation Methods","text":""},{"location":"install/#method-1-install-from-pypi-recommended","title":"Method 1: Install from PyPI (Recommended)","text":"<pre><code>pip install memorix\n</code></pre>"},{"location":"install/#method-2-install-with-optional-dependencies","title":"Method 2: Install with Optional Dependencies","text":"<pre><code># Install with all vector store backends\npip install memorix[all]\n\n# Install with specific backends\npip install memorix[chroma,pinecone,weaviate]\n</code></pre>"},{"location":"install/#method-3-install-from-source","title":"Method 3: Install from Source","text":"<pre><code># Clone the repository\ngit clone https://github.com/memorix-ai/memorix-sdk.git\ncd memorix-sdk\n\n# Install in development mode\npip install -e .\n</code></pre>"},{"location":"install/#vector-store-dependencies","title":"Vector Store Dependencies","text":"<p>Memorix AI supports multiple vector stores. Install the ones you need:</p>"},{"location":"install/#chroma-default","title":"Chroma (Default)","text":"<pre><code>pip install chromadb\n</code></pre>"},{"location":"install/#pinecone","title":"Pinecone","text":"<pre><code>pip install pinecone-client\n</code></pre>"},{"location":"install/#weaviate","title":"Weaviate","text":"<pre><code>pip install weaviate-client\n</code></pre>"},{"location":"install/#qdrant","title":"Qdrant","text":"<pre><code>pip install qdrant-client\n</code></pre>"},{"location":"install/#milvus","title":"Milvus","text":"<pre><code>pip install pymilvus\n</code></pre>"},{"location":"install/#embedding-model-dependencies","title":"Embedding Model Dependencies","text":""},{"location":"install/#openai-embeddings","title":"OpenAI Embeddings","text":"<pre><code>pip install openai\n</code></pre>"},{"location":"install/#hugging-face-transformers","title":"Hugging Face Transformers","text":"<pre><code>pip install transformers torch\n</code></pre>"},{"location":"install/#sentence-transformers","title":"Sentence Transformers","text":"<pre><code>pip install sentence-transformers\n</code></pre>"},{"location":"install/#environment-setup","title":"Environment Setup","text":""},{"location":"install/#1-set-environment-variables","title":"1. Set Environment Variables","text":"<p>Create a <code>.env</code> file in your project directory:</p> <pre><code># OpenAI API Key (for OpenAI embeddings)\nOPENAI_API_KEY=your_openai_api_key_here\n\n# Pinecone API Key (if using Pinecone)\nPINECONE_API_KEY=your_pinecone_api_key_here\nPINECONE_ENVIRONMENT=your_pinecone_environment\n\n# Weaviate API Key (if using Weaviate)\nWEAVIATE_API_KEY=your_weaviate_api_key_here\n</code></pre>"},{"location":"install/#2-load-environment-variables","title":"2. Load Environment Variables","text":"<pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n</code></pre>"},{"location":"install/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code>from memorix import MemoryAPI\n\n# Test basic functionality\ntry:\n    memory = MemoryAPI(\n        vector_store=\"chroma\",\n        embedder=\"sentence-transformers\",\n        collection_name=\"test\"\n    )\n    print(\"\u2705 Memorix AI installed successfully!\")\nexcept Exception as e:\n    print(f\"\u274c Installation issue: {e}\")\n</code></pre>"},{"location":"install/#troubleshooting","title":"Troubleshooting","text":""},{"location":"install/#common-issues","title":"Common Issues","text":""},{"location":"install/#1-import-errors","title":"1. Import Errors","text":"<pre><code># Reinstall with force\npip install --force-reinstall memorix\n</code></pre>"},{"location":"install/#2-vector-store-connection-issues","title":"2. Vector Store Connection Issues","text":"<ul> <li>Check if the vector store service is running</li> <li>Verify API keys and environment variables</li> <li>Ensure network connectivity</li> </ul>"},{"location":"install/#3-memory-issues","title":"3. Memory Issues","text":"<ul> <li>Reduce batch size for large datasets</li> <li>Use smaller embedding models</li> <li>Consider using external vector stores</li> </ul>"},{"location":"install/#4-cudagpu-issues","title":"4. CUDA/GPU Issues","text":"<pre><code># Install CPU-only version\npip install torch --index-url https://download.pytorch.org/whl/cpu\n</code></pre>"},{"location":"install/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the GitHub Issues</li> <li>Search existing discussions</li> <li>Create a new issue with:</li> <li>Python version</li> <li>Operating system</li> <li>Error message</li> <li>Steps to reproduce</li> </ol>"},{"location":"install/#next-steps","title":"Next Steps","text":"<p>After installation, check out:</p> <ul> <li>Quick Start Guide: Get up and running in minutes</li> <li>Basic Usage: Learn the core concepts</li> <li>Examples: See real-world usage patterns </li> </ul>"},{"location":"quickstart/","title":"Quick Start","text":"<p>Get up and running with Memorix AI in under 5 minutes!</p>   ![Memorix AI Quick Start](../img/MEMORIX-AI.png)"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>Memorix AI installed (<code>pip install memorix</code>)</li> <li>Basic Python knowledge</li> </ul>"},{"location":"quickstart/#your-first-memory-system","title":"Your First Memory System","text":""},{"location":"quickstart/#1-basic-setup","title":"1. Basic Setup","text":"<pre><code>from memorix import MemoryAPI\n\n# Create a simple memory system\nmemory = MemoryAPI(\n    vector_store=\"chroma\",  # Use Chroma as vector store\n    embedder=\"sentence-transformers\",  # Use sentence transformers\n    collection_name=\"my_first_memory\"\n)\n</code></pre>"},{"location":"quickstart/#2-store-your-first-memory","title":"2. Store Your First Memory","text":"<pre><code># Store some text with metadata\nmemory.store(\n    text=\"Python is a high-level programming language known for its simplicity and readability.\",\n    metadata={\n        \"topic\": \"programming\",\n        \"language\": \"python\",\n        \"difficulty\": \"beginner\"\n    }\n)\n\n# Store more memories\nmemory.store(\n    text=\"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.\",\n    metadata={\n        \"topic\": \"AI/ML\",\n        \"language\": \"python\",\n        \"difficulty\": \"intermediate\"\n    }\n)\n\nmemory.store(\n    text=\"Vector databases store and retrieve high-dimensional vectors efficiently for similarity search.\",\n    metadata={\n        \"topic\": \"databases\",\n        \"language\": \"python\",\n        \"difficulty\": \"advanced\"\n    }\n)\n</code></pre>"},{"location":"quickstart/#3-retrieve-relevant-memories","title":"3. Retrieve Relevant Memories","text":"<pre><code># Search for programming-related content\nresults = memory.retrieve(\n    query=\"programming languages\",\n    top_k=3\n)\n\nprint(\"Search Results:\")\nfor i, result in enumerate(results, 1):\n    print(f\"{i}. {result.text}\")\n    print(f\"   Metadata: {result.metadata}\")\n    print(f\"   Score: {result.score:.3f}\\n\")\n</code></pre>"},{"location":"quickstart/#4-update-and-delete-memories","title":"4. Update and Delete Memories","text":"<pre><code># Update a memory (if you have the ID)\n# memory.update(id=\"memory_id\", text=\"Updated text\", metadata={\"updated\": True})\n\n# Delete a memory\n# memory.delete(id=\"memory_id\")\n\n# Clear all memories\n# memory.clear()\n</code></pre>"},{"location":"quickstart/#advanced-quick-start","title":"Advanced Quick Start","text":""},{"location":"quickstart/#using-different-vector-stores","title":"Using Different Vector Stores","text":"<pre><code># With Pinecone\nmemory_pinecone = MemoryAPI(\n    vector_store=\"pinecone\",\n    embedder=\"openai\",\n    collection_name=\"pinecone_memories\",\n    vector_store_config={\n        \"api_key\": \"your_pinecone_api_key\",\n        \"environment\": \"your_environment\"\n    }\n)\n\n# With Weaviate\nmemory_weaviate = MemoryAPI(\n    vector_store=\"weaviate\",\n    embedder=\"sentence-transformers\",\n    collection_name=\"weaviate_memories\",\n    vector_store_config={\n        \"url\": \"http://localhost:8080\"\n    }\n)\n</code></pre>"},{"location":"quickstart/#using-different-embedding-models","title":"Using Different Embedding Models","text":"<pre><code># OpenAI embeddings\nmemory_openai = MemoryAPI(\n    vector_store=\"chroma\",\n    embedder=\"openai\",\n    collection_name=\"openai_memories\",\n    embedder_config={\n        \"api_key\": \"your_openai_api_key\"\n    }\n)\n\n# Custom Hugging Face model\nmemory_hf = MemoryAPI(\n    vector_store=\"chroma\",\n    embedder=\"sentence-transformers\",\n    collection_name=\"hf_memories\",\n    embedder_config={\n        \"model_name\": \"all-MiniLM-L6-v2\"\n    }\n)\n</code></pre>"},{"location":"quickstart/#complete-example","title":"Complete Example","text":"<p>Here's a complete example that demonstrates a simple chatbot with memory:</p> <pre><code>from memorix import MemoryAPI\n\nclass ChatbotWithMemory:\n    def __init__(self):\n        self.memory = MemoryAPI(\n            vector_store=\"chroma\",\n            embedder=\"sentence-transformers\",\n            collection_name=\"chatbot_memory\"\n        )\n\n    def chat(self, user_input):\n        # Store user input\n        self.memory.store(\n            text=user_input,\n            metadata={\n                \"type\": \"user_input\",\n                \"timestamp\": \"2024-01-01T12:00:00Z\"\n            }\n        )\n\n        # Retrieve relevant context\n        context = self.memory.retrieve(\n            query=user_input,\n            top_k=3\n        )\n\n        # Generate response (simplified)\n        response = f\"Based on our conversation history, here's what I found relevant: {[r.text for r in context]}\"\n\n        # Store response\n        self.memory.store(\n            text=response,\n            metadata={\n                \"type\": \"bot_response\",\n                \"timestamp\": \"2024-01-01T12:00:01Z\"\n            }\n        )\n\n        return response\n\n# Usage\nchatbot = ChatbotWithMemory()\nresponse = chatbot.chat(\"Tell me about Python programming\")\nprint(response)\n</code></pre>"},{"location":"quickstart/#whats-next","title":"What's Next?","text":"<p>Now that you have the basics, explore:</p> <ul> <li>Basic Usage: Learn about advanced features</li> <li>Memory Management: Understand memory strategies</li> <li>Vector Stores: Compare different storage options</li> <li>Examples: See more complex use cases</li> </ul>"},{"location":"quickstart/#need-help","title":"Need Help?","text":"<ul> <li>Check the API Reference for detailed documentation</li> <li>Join our Discussions</li> <li>Report issues on GitHub </li> </ul>"},{"location":"api/config/","title":"Configuration API","text":"<p>This page will contain the API reference for configuration options.</p>"},{"location":"api/config/#configuration-options","title":"Configuration Options","text":"<p>This documentation is under development.</p>"},{"location":"api/config/#settings","title":"Settings","text":"<ul> <li>Vector Store Configuration</li> <li>Embedder Configuration</li> <li>Memory Settings</li> <li>Performance Options </li> </ul>"},{"location":"api/embedder/","title":"Embedder API","text":"<p>This page will contain the API reference for embedding operations.</p>"},{"location":"api/embedder/#embedder-interface","title":"Embedder Interface","text":"<p>This documentation is under development.</p>"},{"location":"api/embedder/#methods","title":"Methods","text":"<ul> <li><code>embed()</code></li> <li><code>embed_batch()</code></li> <li><code>get_dimension()</code></li> <li><code>get_model_name()</code> </li> </ul>"},{"location":"api/memory_api/","title":"Memory API Reference","text":"<p>Complete reference for the <code>MemoryAPI</code> class and its methods.</p>"},{"location":"api/memory_api/#memoryapi","title":"MemoryAPI","text":"<p>The main interface for interacting with Memorix AI memory systems.</p>"},{"location":"api/memory_api/#constructor","title":"Constructor","text":"<pre><code>MemoryAPI(\n    vector_store: str,\n    embedder: str,\n    collection_name: str,\n    vector_store_config: Optional[Dict] = None,\n    embedder_config: Optional[Dict] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>vector_store</code> (str): Vector store backend (\"chroma\", \"pinecone\", \"weaviate\", etc.)</li> <li><code>embedder</code> (str): Embedding model (\"sentence-transformers\", \"openai\", etc.)</li> <li><code>collection_name</code> (str): Name of the collection to use</li> <li><code>vector_store_config</code> (Dict, optional): Configuration for the vector store</li> <li><code>embedder_config</code> (Dict, optional): Configuration for the embedder</li> </ul> <p>Example:</p> <pre><code>from memorix import MemoryAPI\n\nmemory = MemoryAPI(\n    vector_store=\"chroma\",\n    embedder=\"sentence-transformers\",\n    collection_name=\"my_collection\",\n    vector_store_config={\n        \"persist_directory\": \"./data\"\n    },\n    embedder_config={\n        \"model_name\": \"all-MiniLM-L6-v2\"\n    }\n)\n</code></pre>"},{"location":"api/memory_api/#methods","title":"Methods","text":""},{"location":"api/memory_api/#store","title":"store()","text":"<p>Store a single text with metadata.</p> <pre><code>store(\n    text: str,\n    metadata: Optional[Dict] = None,\n    id: Optional[str] = None\n) -&gt; str\n</code></pre> <p>Parameters:</p> <ul> <li><code>text</code> (str): The text content to store</li> <li><code>metadata</code> (Dict, optional): Additional metadata</li> <li><code>id</code> (str, optional): Custom ID for the memory</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: The ID of the stored memory</li> </ul> <p>Example:</p> <pre><code>memory_id = memory.store(\n    text=\"Python is a programming language\",\n    metadata={\n        \"topic\": \"programming\",\n        \"language\": \"python\"\n    }\n)\nprint(f\"Stored memory with ID: {memory_id}\")\n</code></pre>"},{"location":"api/memory_api/#store_batch","title":"store_batch()","text":"<p>Store multiple texts with metadata in batch.</p> <pre><code>store_batch(\n    texts: List[str],\n    metadatas: Optional[List[Dict]] = None,\n    ids: Optional[List[str]] = None\n) -&gt; List[str]\n</code></pre> <p>Parameters:</p> <ul> <li><code>texts</code> (List[str]): List of text contents to store</li> <li><code>metadatas</code> (List[Dict], optional): List of metadata dictionaries</li> <li><code>ids</code> (List[str], optional): List of custom IDs</li> </ul> <p>Returns:</p> <ul> <li><code>List[str]</code>: List of IDs for the stored memories</li> </ul> <p>Example:</p> <pre><code>texts = [\"Text 1\", \"Text 2\", \"Text 3\"]\nmetadatas = [\n    {\"source\": \"doc1\"},\n    {\"source\": \"doc2\"},\n    {\"source\": \"doc3\"}\n]\n\nmemory_ids = memory.store_batch(texts, metadatas)\nprint(f\"Stored {len(memory_ids)} memories\")\n</code></pre>"},{"location":"api/memory_api/#retrieve","title":"retrieve()","text":"<p>Retrieve memories based on a query.</p> <pre><code>retrieve(\n    query: str,\n    top_k: int = 5,\n    filter: Optional[Dict] = None,\n    score_threshold: Optional[float] = None\n) -&gt; List[MemoryResult]\n</code></pre> <p>Parameters:</p> <ul> <li><code>query</code> (str): The search query</li> <li><code>top_k</code> (int): Number of results to return</li> <li><code>filter</code> (Dict, optional): Metadata filter</li> <li><code>score_threshold</code> (float, optional): Minimum similarity score</li> </ul> <p>Returns:</p> <ul> <li><code>List[MemoryResult]</code>: List of memory results</li> </ul> <p>Example:</p> <pre><code>results = memory.retrieve(\n    query=\"programming languages\",\n    top_k=5,\n    filter={\"category\": \"programming\"},\n    score_threshold=0.7\n)\n\nfor result in results:\n    print(f\"Text: {result.text}\")\n    print(f\"Score: {result.score}\")\n    print(f\"Metadata: {result.metadata}\")\n    print(f\"ID: {result.id}\")\n</code></pre>"},{"location":"api/memory_api/#update","title":"update()","text":"<p>Update an existing memory by ID.</p> <pre><code>update(\n    id: str,\n    text: Optional[str] = None,\n    metadata: Optional[Dict] = None\n) -&gt; bool\n</code></pre> <p>Parameters:</p> <ul> <li><code>id</code> (str): The ID of the memory to update</li> <li><code>text</code> (str, optional): New text content</li> <li><code>metadata</code> (Dict, optional): New metadata</li> </ul> <p>Returns:</p> <ul> <li><code>bool</code>: True if update was successful</li> </ul> <p>Example:</p> <pre><code>success = memory.update(\n    id=\"memory_123\",\n    text=\"Updated text content\",\n    metadata={\"updated\": True, \"timestamp\": \"2024-01-01\"}\n)\n</code></pre>"},{"location":"api/memory_api/#update_by_filter","title":"update_by_filter()","text":"<p>Update memories that match a filter.</p> <pre><code>update_by_filter(\n    filter: Dict,\n    text: Optional[str] = None,\n    metadata: Optional[Dict] = None\n) -&gt; int\n</code></pre> <p>Parameters:</p> <ul> <li><code>filter</code> (Dict): Metadata filter to match memories</li> <li><code>text</code> (str, optional): New text content</li> <li><code>metadata</code> (Dict, optional): New metadata</li> </ul> <p>Returns:</p> <ul> <li><code>int</code>: Number of memories updated</li> </ul> <p>Example:</p> <pre><code>updated_count = memory.update_by_filter(\n    filter={\"category\": \"old_category\"},\n    metadata={\"category\": \"new_category\"}\n)\nprint(f\"Updated {updated_count} memories\")\n</code></pre>"},{"location":"api/memory_api/#delete","title":"delete()","text":"<p>Delete a memory by ID.</p> <pre><code>delete(id: str) -&gt; bool\n</code></pre> <p>Parameters:</p> <ul> <li><code>id</code> (str): The ID of the memory to delete</li> </ul> <p>Returns:</p> <ul> <li><code>bool</code>: True if deletion was successful</li> </ul> <p>Example:</p> <pre><code>success = memory.delete(id=\"memory_123\")\n</code></pre>"},{"location":"api/memory_api/#delete_by_filter","title":"delete_by_filter()","text":"<p>Delete memories that match a filter.</p> <pre><code>delete_by_filter(filter: Dict) -&gt; int\n</code></pre> <p>Parameters:</p> <ul> <li><code>filter</code> (Dict): Metadata filter to match memories</li> </ul> <p>Returns:</p> <ul> <li><code>int</code>: Number of memories deleted</li> </ul> <p>Example:</p> <pre><code>deleted_count = memory.delete_by_filter(\n    filter={\"category\": \"obsolete\"}\n)\nprint(f\"Deleted {deleted_count} memories\")\n</code></pre>"},{"location":"api/memory_api/#clear","title":"clear()","text":"<p>Clear all memories in the collection.</p> <pre><code>clear() -&gt; bool\n</code></pre> <p>Returns:</p> <ul> <li><code>bool</code>: True if clear was successful</li> </ul> <p>Example:</p> <pre><code>success = memory.clear()\n</code></pre>"},{"location":"api/memory_api/#count","title":"count()","text":"<p>Get the number of memories in the collection.</p> <pre><code>count(filter: Optional[Dict] = None) -&gt; int\n</code></pre> <p>Parameters:</p> <ul> <li><code>filter</code> (Dict, optional): Metadata filter</li> </ul> <p>Returns:</p> <ul> <li><code>int</code>: Number of memories</li> </ul> <p>Example:</p> <pre><code>total_count = memory.count()\nfiltered_count = memory.count(filter={\"category\": \"programming\"})\n</code></pre>"},{"location":"api/memory_api/#get_by_id","title":"get_by_id()","text":"<p>Get a specific memory by ID.</p> <pre><code>get_by_id(id: str) -&gt; Optional[MemoryResult]\n</code></pre> <p>Parameters:</p> <ul> <li><code>id</code> (str): The ID of the memory</li> </ul> <p>Returns:</p> <ul> <li><code>Optional[MemoryResult]</code>: Memory result or None if not found</li> </ul> <p>Example:</p> <pre><code>memory_result = memory.get_by_id(\"memory_123\")\nif memory_result:\n    print(f\"Found: {memory_result.text}\")\n</code></pre>"},{"location":"api/memory_api/#list_collections","title":"list_collections()","text":"<p>List all available collections.</p> <pre><code>list_collections() -&gt; List[str]\n</code></pre> <p>Returns:</p> <ul> <li><code>List[str]</code>: List of collection names</li> </ul> <p>Example:</p> <pre><code>collections = memory.list_collections()\nprint(f\"Available collections: {collections}\")\n</code></pre>"},{"location":"api/memory_api/#memoryresult","title":"MemoryResult","text":"<p>The result object returned by retrieval operations.</p>"},{"location":"api/memory_api/#attributes","title":"Attributes","text":"<ul> <li><code>id</code> (str): Memory ID</li> <li><code>text</code> (str): Text content</li> <li><code>metadata</code> (Dict): Metadata dictionary</li> <li><code>score</code> (float): Similarity score</li> <li><code>embedding</code> (Optional[np.ndarray]): Vector embedding</li> </ul>"},{"location":"api/memory_api/#example","title":"Example","text":"<pre><code>results = memory.retrieve(\"python programming\", top_k=1)\nif results:\n    result = results[0]\n    print(f\"ID: {result.id}\")\n    print(f\"Text: {result.text}\")\n    print(f\"Score: {result.score}\")\n    print(f\"Metadata: {result.metadata}\")\n</code></pre>"},{"location":"api/memory_api/#error-handling","title":"Error Handling","text":""},{"location":"api/memory_api/#common-exceptions","title":"Common Exceptions","text":"<pre><code>from memorix import MemoryAPI, MemoryError\n\ntry:\n    memory = MemoryAPI(\n        vector_store=\"invalid_store\",\n        embedder=\"sentence-transformers\",\n        collection_name=\"test\"\n    )\nexcept MemoryError as e:\n    print(f\"Memory error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"api/memory_api/#connection-errors","title":"Connection Errors","text":"<pre><code>try:\n    results = memory.retrieve(\"query\")\nexcept ConnectionError:\n    print(\"Vector store connection failed\")\n    # Implement fallback logic\n</code></pre>"},{"location":"api/memory_api/#configuration-examples","title":"Configuration Examples","text":""},{"location":"api/memory_api/#chroma-configuration","title":"Chroma Configuration","text":"<pre><code>memory = MemoryAPI(\n    vector_store=\"chroma\",\n    embedder=\"sentence-transformers\",\n    collection_name=\"my_collection\",\n    vector_store_config={\n        \"persist_directory\": \"./chroma_db\",\n        \"anonymized_telemetry\": False,\n        \"allow_reset\": True\n    }\n)\n</code></pre>"},{"location":"api/memory_api/#pinecone-configuration","title":"Pinecone Configuration","text":"<pre><code>memory = MemoryAPI(\n    vector_store=\"pinecone\",\n    embedder=\"openai\",\n    collection_name=\"my_collection\",\n    vector_store_config={\n        \"api_key\": \"your_pinecone_api_key\",\n        \"environment\": \"us-west1-gcp\",\n        \"dimension\": 1536,\n        \"metric\": \"cosine\"\n    }\n)\n</code></pre>"},{"location":"api/memory_api/#openai-embedder-configuration","title":"OpenAI Embedder Configuration","text":"<pre><code>memory = MemoryAPI(\n    vector_store=\"chroma\",\n    embedder=\"openai\",\n    collection_name=\"my_collection\",\n    embedder_config={\n        \"api_key\": \"your_openai_api_key\",\n        \"model\": \"text-embedding-ada-002\",\n        \"chunk_size\": 1000\n    }\n)\n</code></pre>"},{"location":"api/memory_api/#performance-tips","title":"Performance Tips","text":""},{"location":"api/memory_api/#batch-operations","title":"Batch Operations","text":"<pre><code># Use batch operations for large datasets\nBATCH_SIZE = 100\ntexts = [\"Text 1\", \"Text 2\", ...]  # Large list\n\nfor i in range(0, len(texts), BATCH_SIZE):\n    batch = texts[i:i + BATCH_SIZE]\n    memory.store_batch(batch)\n</code></pre>"},{"location":"api/memory_api/#filtering","title":"Filtering","text":"<pre><code># Use filters to improve performance\nresults = memory.retrieve(\n    query=\"search\",\n    filter={\"recent\": True}  # Only search recent items\n)\n</code></pre>"},{"location":"api/memory_api/#score-thresholds","title":"Score Thresholds","text":"<pre><code># Use score thresholds to get only relevant results\nresults = memory.retrieve(\n    query=\"search\",\n    score_threshold=0.8  # Only results with 80%+ similarity\n)\n</code></pre>"},{"location":"api/vector_store/","title":"Vector Store API","text":"<p>This page will contain the API reference for vector store operations.</p>"},{"location":"api/vector_store/#vector-store-interface","title":"Vector Store Interface","text":"<p>This documentation is under development.</p>"},{"location":"api/vector_store/#methods","title":"Methods","text":"<ul> <li><code>connect()</code></li> <li><code>disconnect()</code></li> <li><code>store()</code></li> <li><code>retrieve()</code></li> <li><code>delete()</code> </li> </ul>"},{"location":"contributing/ci-cd/","title":"CI/CD and Deployment","text":"<p>This document explains the Continuous Integration and Continuous Deployment (CI/CD) setup for the Memorix AI documentation.</p>"},{"location":"contributing/ci-cd/#overview","title":"Overview","text":"<p>The documentation uses GitHub Actions for automated testing, building, and deployment. The CI/CD pipeline ensures that:</p> <ul> <li>All changes are tested before deployment</li> <li>Documentation is automatically built and deployed</li> <li>Preview environments are created for pull requests</li> <li>Dependencies are kept up to date</li> </ul>"},{"location":"contributing/ci-cd/#workflows","title":"Workflows","text":""},{"location":"contributing/ci-cd/#1-test-documentation-githubworkflowstestyml","title":"1. Test Documentation (<code>.github/workflows/test.yml</code>)","text":"<p>Triggers: - Push to <code>main</code>, <code>master</code>, or <code>develop</code> branches - Pull requests to <code>main</code> or <code>master</code></p> <p>What it does: - Installs Python dependencies - Builds the documentation - Validates markdown files - Checks for broken links - Ensures file structure is correct - Validates MkDocs configuration</p>"},{"location":"contributing/ci-cd/#2-deploy-documentation-githubworkflowsdeployyml","title":"2. Deploy Documentation (<code>.github/workflows/deploy.yml</code>)","text":"<p>Triggers: - Push to <code>main</code> or <code>master</code> branches - Pull requests to <code>main</code> or <code>master</code></p> <p>What it does: - Builds the documentation - Deploys to GitHub Pages (only on main/master) - Creates deployment artifacts</p>"},{"location":"contributing/ci-cd/#3-preview-documentation-githubworkflowspreviewyml","title":"3. Preview Documentation (<code>.github/workflows/preview.yml</code>)","text":"<p>Triggers: - Pull requests to <code>main</code> or <code>master</code></p> <p>What it does: - Creates a preview deployment - Comments on the PR with the preview URL - Allows reviewers to see changes before merging</p>"},{"location":"contributing/ci-cd/#4-dependabot-auto-merge-githubworkflowsdependabotyml","title":"4. Dependabot Auto-merge (<code>.github/workflows/dependabot.yml</code>)","text":"<p>Triggers: - Pull requests from Dependabot</p> <p>What it does: - Automatically merges patch updates - Runs tests for minor/major updates - Ensures dependency updates are safe</p>"},{"location":"contributing/ci-cd/#deployment-process","title":"Deployment Process","text":""},{"location":"contributing/ci-cd/#automatic-deployment","title":"Automatic Deployment","text":"<ol> <li>Push to main branch \u2192 Triggers build and deploy</li> <li>GitHub Actions builds \u2192 Documentation using MkDocs</li> <li>Deploy to GitHub Pages \u2192 Site goes live at <code>https://memorix-ai.github.io/memorix-docs/</code></li> </ol>"},{"location":"contributing/ci-cd/#manual-deployment","title":"Manual Deployment","text":"<p>You can also deploy manually using the provided script:</p> <pre><code># Build and deploy\n./deploy.sh\n\n# Or use MkDocs directly\nmkdocs build\nmkdocs gh-deploy\n</code></pre>"},{"location":"contributing/ci-cd/#environment-setup","title":"Environment Setup","text":""},{"location":"contributing/ci-cd/#required-github-settings","title":"Required GitHub Settings","text":"<ol> <li>Enable GitHub Pages:</li> <li>Go to Settings \u2192 Pages</li> <li>Source: \"GitHub Actions\"</li> <li> <p>Branch: Leave empty (handled by workflow)</p> </li> <li> <p>Enable GitHub Actions:</p> </li> <li>Go to Settings \u2192 Actions \u2192 General</li> <li> <p>Allow all actions and reusable workflows</p> </li> <li> <p>Set up permissions:</p> </li> <li>Go to Settings \u2192 Actions \u2192 General</li> <li>Enable \"Read and write permissions\"</li> <li>Enable \"Allow GitHub Actions to create and approve pull requests\"</li> </ol>"},{"location":"contributing/ci-cd/#required-secrets","title":"Required Secrets","text":"<p>No additional secrets are required for basic deployment. The workflows use the default <code>GITHUB_TOKEN</code>.</p>"},{"location":"contributing/ci-cd/#monitoring-and-troubleshooting","title":"Monitoring and Troubleshooting","text":""},{"location":"contributing/ci-cd/#check-workflow-status","title":"Check Workflow Status","text":"<ol> <li>Go to the Actions tab in your repository</li> <li>View the status of recent workflow runs</li> <li>Click on a workflow to see detailed logs</li> </ol>"},{"location":"contributing/ci-cd/#common-issues","title":"Common Issues","text":""},{"location":"contributing/ci-cd/#build-failures","title":"Build Failures","text":"<pre><code># Check locally\nmkdocs build\n\n# Check configuration\nmkdocs config\n</code></pre>"},{"location":"contributing/ci-cd/#deployment-failures","title":"Deployment Failures","text":"<ol> <li>Check GitHub Pages settings</li> <li>Verify repository permissions</li> <li>Check workflow logs for specific errors</li> </ol>"},{"location":"contributing/ci-cd/#preview-not-working","title":"Preview Not Working","text":"<ol> <li>Ensure the preview workflow is enabled</li> <li>Check that the PR is targeting the correct branch</li> <li>Verify GitHub Pages is configured correctly</li> </ol>"},{"location":"contributing/ci-cd/#best-practices","title":"Best Practices","text":""},{"location":"contributing/ci-cd/#for-contributors","title":"For Contributors","text":"<ol> <li> <p>Always test locally: <pre><code>mkdocs serve\n</code></pre></p> </li> <li> <p>Check your changes:</p> </li> <li>Validate markdown syntax</li> <li>Test internal links</li> <li> <p>Ensure images are properly referenced</p> </li> <li> <p>Use descriptive commit messages: <pre><code>git commit -m \"docs: add new API reference section\"\n</code></pre></p> </li> </ol>"},{"location":"contributing/ci-cd/#for-maintainers","title":"For Maintainers","text":"<ol> <li>Review PRs thoroughly:</li> <li>Check the preview deployment</li> <li>Validate all links work</li> <li> <p>Ensure consistent formatting</p> </li> <li> <p>Monitor dependency updates:</p> </li> <li>Review Dependabot PRs</li> <li>Test major version updates</li> <li> <p>Keep dependencies current</p> </li> <li> <p>Regular maintenance:</p> </li> <li>Update MkDocs and themes</li> <li>Review and update documentation</li> <li>Monitor workflow performance</li> </ol>"},{"location":"contributing/ci-cd/#customization","title":"Customization","text":""},{"location":"contributing/ci-cd/#adding-new-workflows","title":"Adding New Workflows","text":"<p>To add a new workflow:</p> <ol> <li>Create a new <code>.yml</code> file in <code>.github/workflows/</code></li> <li>Define triggers and jobs</li> <li>Test locally using <code>act</code> (optional)</li> </ol>"},{"location":"contributing/ci-cd/#modifying-existing-workflows","title":"Modifying Existing Workflows","text":"<ol> <li>Edit the workflow file</li> <li>Test changes in a branch</li> <li>Create a PR to merge changes</li> </ol>"},{"location":"contributing/ci-cd/#environment-variables","title":"Environment Variables","text":"<p>You can add environment variables in the workflow files:</p> <pre><code>env:\n  CUSTOM_VAR: \"value\"\n</code></pre>"},{"location":"contributing/ci-cd/#security","title":"Security","text":""},{"location":"contributing/ci-cd/#permissions","title":"Permissions","text":"<p>The workflows use minimal required permissions: - <code>contents: read</code> - Read repository content - <code>pages: write</code> - Deploy to GitHub Pages - <code>id-token: write</code> - Generate deployment tokens</p>"},{"location":"contributing/ci-cd/#dependencies","title":"Dependencies","text":"<p>All dependencies are pinned to specific versions in <code>requirements.txt</code> to ensure reproducible builds.</p>"},{"location":"contributing/ci-cd/#support","title":"Support","text":"<p>If you encounter issues with the CI/CD setup:</p> <ol> <li>Check the GitHub Actions documentation</li> <li>Review workflow logs for specific errors</li> <li>Create an issue in the repository</li> <li>Contact the maintainers</li> </ol>"},{"location":"contributing/ci-cd/#related-documentation","title":"Related Documentation","text":"<ul> <li>Contributing Guide</li> <li>Installation Guide</li> <li>API Reference </li> </ul>"},{"location":"examples/basic/","title":"Basic Examples","text":"<p>This page will contain basic usage examples for Memorix AI.</p>"},{"location":"examples/basic/#getting-started-examples","title":"Getting Started Examples","text":"<p>This documentation is under development.</p>"},{"location":"examples/basic/#examples","title":"Examples","text":"<ul> <li>Simple Memory Storage</li> <li>Basic Retrieval</li> <li>Metadata Usage</li> <li>Batch Operations </li> </ul>"},{"location":"examples/chatbot/","title":"Chatbot Memory Examples","text":"<p>This page will contain examples of using Memorix AI with chatbots.</p>"},{"location":"examples/chatbot/#chatbot-integration","title":"Chatbot Integration","text":"<p>This documentation is under development.</p>"},{"location":"examples/chatbot/#examples","title":"Examples","text":"<ul> <li>Conversation Memory</li> <li>Context Retrieval</li> <li>Session Management</li> <li>Multi-turn Dialogues </li> </ul>"},{"location":"examples/knowledge_base/","title":"Knowledge Base Examples","text":"<p>This page will contain examples of using Memorix AI for knowledge base applications.</p>"},{"location":"examples/knowledge_base/#knowledge-base-applications","title":"Knowledge Base Applications","text":"<p>This documentation is under development.</p>"},{"location":"examples/knowledge_base/#examples","title":"Examples","text":"<ul> <li>Document Indexing</li> <li>Semantic Search</li> <li>Question Answering</li> <li>Information Retrieval </li> </ul>"},{"location":"usage/basic/","title":"Basic Usage","text":"<p>Learn the core concepts and common patterns for using Memorix AI effectively.</p> <p></p>"},{"location":"usage/basic/#core-concepts","title":"Core Concepts","text":""},{"location":"usage/basic/#memoryapi","title":"MemoryAPI","text":"<p>The <code>MemoryAPI</code> class is the main interface for interacting with Memorix AI. It provides a unified way to work with different vector stores and embedding models.</p> <pre><code>from memorix import MemoryAPI\n\nmemory = MemoryAPI(\n    vector_store=\"chroma\",      # Vector store backend\n    embedder=\"sentence-transformers\",  # Embedding model\n    collection_name=\"my_collection\"    # Collection name\n)\n</code></pre>"},{"location":"usage/basic/#key-components","title":"Key Components","text":"<ol> <li>Vector Store: Stores and retrieves vector embeddings</li> <li>Embedder: Converts text to vector embeddings</li> <li>Collection: A logical grouping of related memories</li> <li>Metadata: Additional information stored with each memory</li> </ol>"},{"location":"usage/basic/#basic-operations","title":"Basic Operations","text":""},{"location":"usage/basic/#storing-memories","title":"Storing Memories","text":"<pre><code># Simple storage\nmemory.store(\n    text=\"Your text content here\",\n    metadata={\"key\": \"value\"}\n)\n\n# Batch storage\ntexts = [\"Text 1\", \"Text 2\", \"Text 3\"]\nmetadatas = [\n    {\"source\": \"doc1\"},\n    {\"source\": \"doc2\"},\n    {\"source\": \"doc3\"}\n]\n\nmemory.store_batch(texts, metadatas)\n</code></pre>"},{"location":"usage/basic/#retrieving-memories","title":"Retrieving Memories","text":"<pre><code># Basic retrieval\nresults = memory.retrieve(\n    query=\"search query\",\n    top_k=5\n)\n\n# Retrieval with filters\nresults = memory.retrieve(\n    query=\"search query\",\n    top_k=5,\n    filter={\"category\": \"programming\"}\n)\n\n# Retrieval with score threshold\nresults = memory.retrieve(\n    query=\"search query\",\n    top_k=5,\n    score_threshold=0.7\n)\n</code></pre>"},{"location":"usage/basic/#updating-memories","title":"Updating Memories","text":"<pre><code># Update by ID\nmemory.update(\n    id=\"memory_id\",\n    text=\"Updated text\",\n    metadata={\"updated\": True}\n)\n\n# Update by filter\nmemory.update_by_filter(\n    filter={\"category\": \"old_category\"},\n    text=\"Updated text\",\n    metadata={\"category\": \"new_category\"}\n)\n</code></pre>"},{"location":"usage/basic/#deleting-memories","title":"Deleting Memories","text":"<pre><code># Delete by ID\nmemory.delete(id=\"memory_id\")\n\n# Delete by filter\nmemory.delete_by_filter(filter={\"category\": \"obsolete\"})\n\n# Clear all memories\nmemory.clear()\n</code></pre>"},{"location":"usage/basic/#advanced-features","title":"Advanced Features","text":""},{"location":"usage/basic/#metadata-filtering","title":"Metadata Filtering","text":"<pre><code># Filter by exact match\nresults = memory.retrieve(\n    query=\"python\",\n    filter={\"language\": \"python\"}\n)\n\n# Filter by multiple conditions\nresults = memory.retrieve(\n    query=\"programming\",\n    filter={\n        \"language\": \"python\",\n        \"difficulty\": {\"$in\": [\"beginner\", \"intermediate\"]}\n    }\n)\n\n# Filter by date range\nresults = memory.retrieve(\n    query=\"recent topics\",\n    filter={\n        \"timestamp\": {\n            \"$gte\": \"2024-01-01\",\n            \"$lte\": \"2024-12-31\"\n        }\n    }\n)\n</code></pre>"},{"location":"usage/basic/#batch-operations","title":"Batch Operations","text":"<pre><code># Batch store with progress tracking\nfrom tqdm import tqdm\n\ntexts = [\"Text 1\", \"Text 2\", \"Text 3\", ...]\nmetadatas = [{\"id\": i} for i in range(len(texts))]\n\nfor i in tqdm(range(0, len(texts), 100)):\n    batch_texts = texts[i:i+100]\n    batch_metadatas = metadatas[i:i+100]\n    memory.store_batch(batch_texts, batch_metadatas)\n</code></pre>"},{"location":"usage/basic/#custom-embeddings","title":"Custom Embeddings","text":"<pre><code># Use pre-computed embeddings\nimport numpy as np\n\nembeddings = np.random.rand(3, 384)  # 3 vectors of dimension 384\ntexts = [\"Text 1\", \"Text 2\", \"Text 3\"]\nmetadatas = [{\"custom\": True} for _ in texts]\n\nmemory.store_with_embeddings(texts, embeddings, metadatas)\n</code></pre>"},{"location":"usage/basic/#configuration-options","title":"Configuration Options","text":""},{"location":"usage/basic/#vector-store-configuration","title":"Vector Store Configuration","text":"<pre><code># Chroma configuration\nmemory = MemoryAPI(\n    vector_store=\"chroma\",\n    embedder=\"sentence-transformers\",\n    collection_name=\"my_collection\",\n    vector_store_config={\n        \"persist_directory\": \"./chroma_db\",\n        \"anonymized_telemetry\": False\n    }\n)\n\n# Pinecone configuration\nmemory = MemoryAPI(\n    vector_store=\"pinecone\",\n    embedder=\"openai\",\n    collection_name=\"my_collection\",\n    vector_store_config={\n        \"api_key\": \"your_api_key\",\n        \"environment\": \"us-west1-gcp\",\n        \"dimension\": 1536\n    }\n)\n</code></pre>"},{"location":"usage/basic/#embedder-configuration","title":"Embedder Configuration","text":"<pre><code># Sentence Transformers configuration\nmemory = MemoryAPI(\n    vector_store=\"chroma\",\n    embedder=\"sentence-transformers\",\n    collection_name=\"my_collection\",\n    embedder_config={\n        \"model_name\": \"all-MiniLM-L6-v2\",\n        \"device\": \"cuda\"  # or \"cpu\"\n    }\n)\n\n# OpenAI configuration\nmemory = MemoryAPI(\n    vector_store=\"chroma\",\n    embedder=\"openai\",\n    collection_name=\"my_collection\",\n    embedder_config={\n        \"api_key\": \"your_openai_api_key\",\n        \"model\": \"text-embedding-ada-002\"\n    }\n)\n</code></pre>"},{"location":"usage/basic/#best-practices","title":"Best Practices","text":""},{"location":"usage/basic/#1-collection-naming","title":"1. Collection Naming","text":"<pre><code># Use descriptive collection names\nmemory = MemoryAPI(\n    vector_store=\"chroma\",\n    embedder=\"sentence-transformers\",\n    collection_name=\"customer_support_2024\"\n)\n</code></pre>"},{"location":"usage/basic/#2-metadata-design","title":"2. Metadata Design","text":"<pre><code># Use consistent metadata structure\nmetadata = {\n    \"source\": \"document_name\",\n    \"timestamp\": \"2024-01-01T12:00:00Z\",\n    \"category\": \"topic_category\",\n    \"user_id\": \"user_identifier\",\n    \"version\": \"1.0\"\n}\n</code></pre>"},{"location":"usage/basic/#3-error-handling","title":"3. Error Handling","text":"<pre><code>try:\n    results = memory.retrieve(query=\"search\", top_k=5)\nexcept Exception as e:\n    print(f\"Retrieval failed: {e}\")\n    # Fallback to default behavior\n    results = []\n</code></pre>"},{"location":"usage/basic/#4-performance-optimization","title":"4. Performance Optimization","text":"<pre><code># Use batch operations for large datasets\n# Set appropriate batch sizes\nBATCH_SIZE = 100\n\n# Use filters to reduce search space\nresults = memory.retrieve(\n    query=\"search\",\n    top_k=5,\n    filter={\"recent\": True}  # Only search recent items\n)\n</code></pre>"},{"location":"usage/basic/#common-patterns","title":"Common Patterns","text":""},{"location":"usage/basic/#conversation-memory","title":"Conversation Memory","text":"<pre><code>class ConversationMemory:\n    def __init__(self, user_id):\n        self.memory = MemoryAPI(\n            vector_store=\"chroma\",\n            embedder=\"sentence-transformers\",\n            collection_name=f\"conversation_{user_id}\"\n        )\n\n    def add_message(self, message, role=\"user\"):\n        self.memory.store(\n            text=message,\n            metadata={\n                \"role\": role,\n                \"timestamp\": datetime.now().isoformat()\n            }\n        )\n\n    def get_context(self, query, top_k=5):\n        return self.memory.retrieve(query, top_k=top_k)\n</code></pre>"},{"location":"usage/basic/#document-search","title":"Document Search","text":"<pre><code>class DocumentSearch:\n    def __init__(self):\n        self.memory = MemoryAPI(\n            vector_store=\"chroma\",\n            embedder=\"sentence-transformers\",\n            collection_name=\"documents\"\n        )\n\n    def index_document(self, doc_id, content, metadata=None):\n        self.memory.store(\n            text=content,\n            metadata={\n                \"doc_id\": doc_id,\n                **(metadata or {})\n            }\n        )\n\n    def search_documents(self, query, top_k=10):\n        return self.memory.retrieve(query, top_k=top_k)\n</code></pre>"},{"location":"usage/basic/#next-steps","title":"Next Steps","text":"<ul> <li>Memory Management: Learn about advanced memory strategies</li> <li>Vector Stores: Compare different storage backends</li> <li>Embedding Models: Understand embedding options</li> <li>API Reference: Complete API documentation </li> </ul>"},{"location":"usage/embeddings/","title":"Embedding Models","text":"<p>This page will contain documentation about different embedding models supported by Memorix AI.</p>"},{"location":"usage/embeddings/#supported-embedding-models","title":"Supported Embedding Models","text":"<ul> <li>OpenAI Embeddings</li> <li>Sentence Transformers</li> <li>Hugging Face Models</li> <li>Custom Embeddings</li> </ul> <p>This documentation is under development. </p>"},{"location":"usage/memory/","title":"Memory Management","text":"<p>Learn about advanced memory management strategies and best practices for building effective AI memory systems.</p>"},{"location":"usage/memory/#memory-strategies","title":"Memory Strategies","text":""},{"location":"usage/memory/#1-conversation-memory","title":"1. Conversation Memory","text":"<p>Store and retrieve conversation history for context-aware responses.</p> <pre><code>from memorix import MemoryAPI\nfrom datetime import datetime\n\nclass ConversationMemory:\n    def __init__(self, user_id):\n        self.memory = MemoryAPI(\n            vector_store=\"chroma\",\n            embedder=\"sentence-transformers\",\n            collection_name=f\"conversation_{user_id}\"\n        )\n\n    def add_message(self, message, role=\"user\", session_id=None):\n        \"\"\"Add a message to the conversation memory.\"\"\"\n        self.memory.store(\n            text=message,\n            metadata={\n                \"role\": role,\n                \"session_id\": session_id,\n                \"timestamp\": datetime.now().isoformat(),\n                \"type\": \"conversation\"\n            }\n        )\n\n    def get_context(self, query, top_k=5, session_id=None):\n        \"\"\"Retrieve relevant conversation context.\"\"\"\n        filter_dict = {\"type\": \"conversation\"}\n        if session_id:\n            filter_dict[\"session_id\"] = session_id\n\n        return self.memory.retrieve(\n            query=query,\n            top_k=top_k,\n            filter=filter_dict\n        )\n\n    def get_recent_context(self, hours=24, top_k=10):\n        \"\"\"Get recent conversation context.\"\"\"\n        from datetime import datetime, timedelta\n\n        cutoff_time = (datetime.now() - timedelta(hours=hours)).isoformat()\n\n        return self.memory.retrieve(\n            query=\"recent conversation\",\n            top_k=top_k,\n            filter={\n                \"type\": \"conversation\",\n                \"timestamp\": {\"$gte\": cutoff_time}\n            }\n        )\n</code></pre>"},{"location":"usage/memory/#2-knowledge-base-memory","title":"2. Knowledge Base Memory","text":"<p>Build and maintain a knowledge base for domain-specific information.</p> <pre><code>class KnowledgeBase:\n    def __init__(self, domain):\n        self.memory = MemoryAPI(\n            vector_store=\"chroma\",\n            embedder=\"sentence-transformers\",\n            collection_name=f\"knowledge_{domain}\"\n        )\n\n    def add_knowledge(self, content, category, source=None, tags=None):\n        \"\"\"Add knowledge to the base.\"\"\"\n        metadata = {\n            \"type\": \"knowledge\",\n            \"category\": category,\n            \"source\": source,\n            \"tags\": tags or [],\n            \"timestamp\": datetime.now().isoformat()\n        }\n\n        return self.memory.store(text=content, metadata=metadata)\n\n    def search_knowledge(self, query, category=None, tags=None, top_k=5):\n        \"\"\"Search knowledge base.\"\"\"\n        filter_dict = {\"type\": \"knowledge\"}\n\n        if category:\n            filter_dict[\"category\"] = category\n\n        if tags:\n            filter_dict[\"tags\"] = {\"$in\": tags}\n\n        return self.memory.retrieve(\n            query=query,\n            top_k=top_k,\n            filter=filter_dict\n        )\n\n    def update_knowledge(self, id, content, category=None, tags=None):\n        \"\"\"Update existing knowledge.\"\"\"\n        metadata = {}\n        if category:\n            metadata[\"category\"] = category\n        if tags:\n            metadata[\"tags\"] = tags\n\n        return self.memory.update(id=id, text=content, metadata=metadata)\n</code></pre>"},{"location":"usage/memory/#3-episodic-memory","title":"3. Episodic Memory","text":"<p>Store and retrieve specific events or experiences.</p> <pre><code>class EpisodicMemory:\n    def __init__(self):\n        self.memory = MemoryAPI(\n            vector_store=\"chroma\",\n            embedder=\"sentence-transformers\",\n            collection_name=\"episodic_memory\"\n        )\n\n    def store_episode(self, event, location=None, participants=None, emotions=None):\n        \"\"\"Store an episodic memory.\"\"\"\n        metadata = {\n            \"type\": \"episode\",\n            \"location\": location,\n            \"participants\": participants or [],\n            \"emotions\": emotions or [],\n            \"timestamp\": datetime.now().isoformat()\n        }\n\n        return self.memory.store(text=event, metadata=metadata)\n\n    def recall_episodes(self, query, location=None, participants=None, top_k=5):\n        \"\"\"Recall relevant episodes.\"\"\"\n        filter_dict = {\"type\": \"episode\"}\n\n        if location:\n            filter_dict[\"location\"] = location\n\n        if participants:\n            filter_dict[\"participants\"] = {\"$in\": participants}\n\n        return self.memory.retrieve(\n            query=query,\n            top_k=top_k,\n            filter=filter_dict\n        )\n</code></pre>"},{"location":"usage/memory/#memory-lifecycle-management","title":"Memory Lifecycle Management","text":""},{"location":"usage/memory/#1-memory-retention-policies","title":"1. Memory Retention Policies","text":"<p>Implement policies for managing memory retention and cleanup.</p> <pre><code>class MemoryManager:\n    def __init__(self, collection_name):\n        self.memory = MemoryAPI(\n            vector_store=\"chroma\",\n            embedder=\"sentence-transformers\",\n            collection_name=collection_name\n        )\n\n    def cleanup_old_memories(self, days_old=30):\n        \"\"\"Remove memories older than specified days.\"\"\"\n        from datetime import datetime, timedelta\n\n        cutoff_date = (datetime.now() - timedelta(days=days_old)).isoformat()\n\n        deleted_count = self.memory.delete_by_filter(\n            filter={\"timestamp\": {\"$lt\": cutoff_date}}\n        )\n\n        print(f\"Cleaned up {deleted_count} old memories\")\n        return deleted_count\n\n    def archive_memories(self, filter_dict, archive_collection):\n        \"\"\"Archive memories to a separate collection.\"\"\"\n        # Retrieve memories to archive\n        memories = self.memory.retrieve(\n            query=\"\",  # Empty query to get all matching\n            top_k=1000,  # Large number to get all\n            filter=filter_dict\n        )\n\n        # Store in archive collection\n        archive_memory = MemoryAPI(\n            vector_store=\"chroma\",\n            embedder=\"sentence-transformers\",\n            collection_name=archive_collection\n        )\n\n        for memory in memories:\n            archive_memory.store(\n                text=memory.text,\n                metadata={**memory.metadata, \"archived\": True}\n            )\n\n        # Delete from original collection\n        self.memory.delete_by_filter(filter=filter_dict)\n\n        return len(memories)\n\n    def get_memory_stats(self):\n        \"\"\"Get statistics about the memory collection.\"\"\"\n        total_count = self.memory.count()\n\n        # Count by type\n        conversation_count = self.memory.count(filter={\"type\": \"conversation\"})\n        knowledge_count = self.memory.count(filter={\"type\": \"knowledge\"})\n        episode_count = self.memory.count(filter={\"type\": \"episode\"})\n\n        return {\n            \"total\": total_count,\n            \"conversations\": conversation_count,\n            \"knowledge\": knowledge_count,\n            \"episodes\": episode_count\n        }\n</code></pre>"},{"location":"usage/memory/#2-memory-compression","title":"2. Memory Compression","text":"<p>Compress and summarize memories to save space while preserving important information.</p> <pre><code>class MemoryCompressor:\n    def __init__(self, memory_api):\n        self.memory = memory_api\n\n    def compress_conversation(self, session_id, max_memories=50):\n        \"\"\"Compress conversation memories by summarizing.\"\"\"\n        # Get all memories for the session\n        memories = self.memory.retrieve(\n            query=\"\",\n            top_k=max_memories,\n            filter={\"session_id\": session_id, \"type\": \"conversation\"}\n        )\n\n        if len(memories) &lt;= max_memories:\n            return  # No compression needed\n\n        # Group memories by time windows\n        from datetime import datetime, timedelta\n\n        # Create summary for each time window\n        summaries = []\n        window_size = timedelta(hours=1)\n\n        # Implementation would group memories by time windows\n        # and create summaries for each window\n\n        # Store summaries and delete original memories\n        for summary in summaries:\n            self.memory.store(\n                text=summary[\"text\"],\n                metadata={\n                    **summary[\"metadata\"],\n                    \"compressed\": True,\n                    \"original_count\": summary[\"original_count\"]\n                }\n            )\n\n        # Delete original memories\n        self.memory.delete_by_filter(\n            filter={\"session_id\": session_id, \"type\": \"conversation\", \"compressed\": {\"$ne\": True}}\n        )\n</code></pre>"},{"location":"usage/memory/#memory-optimization","title":"Memory Optimization","text":""},{"location":"usage/memory/#1-batch-operations","title":"1. Batch Operations","text":"<p>Use batch operations for better performance with large datasets.</p> <pre><code>def batch_store_memories(memory_api, memories_data, batch_size=100):\n    \"\"\"Store memories in batches for better performance.\"\"\"\n    from tqdm import tqdm\n\n    total_memories = len(memories_data)\n\n    for i in tqdm(range(0, total_memories, batch_size), desc=\"Storing memories\"):\n        batch = memories_data[i:i + batch_size]\n\n        texts = [item[\"text\"] for item in batch]\n        metadatas = [item[\"metadata\"] for item in batch]\n\n        memory_api.store_batch(texts, metadatas)\n</code></pre>"},{"location":"usage/memory/#2-memory-indexing","title":"2. Memory Indexing","text":"<p>Create efficient indexes for frequently queried metadata.</p> <pre><code>def create_memory_indexes(memory_api):\n    \"\"\"Create indexes for common query patterns.\"\"\"\n    # This would depend on the specific vector store implementation\n    # For Chroma, indexes are created automatically\n    # For other stores, you might need to create explicit indexes\n\n    # Example for metadata fields that are frequently filtered\n    common_filters = [\"type\", \"category\", \"user_id\", \"session_id\"]\n\n    print(\"Memory indexes created for:\", common_filters)\n</code></pre>"},{"location":"usage/memory/#3-memory-caching","title":"3. Memory Caching","text":"<p>Implement caching for frequently accessed memories.</p> <pre><code>from functools import lru_cache\nimport hashlib\n\nclass CachedMemoryAPI:\n    def __init__(self, memory_api, cache_size=1000):\n        self.memory = memory_api\n        self.cache_size = cache_size\n\n    @lru_cache(maxsize=1000)\n    def cached_retrieve(self, query_hash, top_k, filter_hash):\n        \"\"\"Cached version of retrieve method.\"\"\"\n        # Convert hashes back to original values\n        # This is a simplified example\n        return self.memory.retrieve(query_hash, top_k, filter_hash)\n\n    def retrieve(self, query, top_k=5, filter=None):\n        \"\"\"Retrieve with caching.\"\"\"\n        # Create hash for caching\n        query_hash = hashlib.md5(query.encode()).hexdigest()\n        filter_hash = hashlib.md5(str(filter).encode()).hexdigest()\n\n        return self.cached_retrieve(query_hash, top_k, filter_hash)\n</code></pre>"},{"location":"usage/memory/#best-practices","title":"Best Practices","text":""},{"location":"usage/memory/#1-memory-organization","title":"1. Memory Organization","text":"<ul> <li>Use consistent metadata schemas</li> <li>Implement hierarchical organization</li> <li>Use tags for flexible categorization</li> <li>Maintain clear naming conventions</li> </ul>"},{"location":"usage/memory/#2-performance-optimization","title":"2. Performance Optimization","text":"<ul> <li>Use appropriate batch sizes</li> <li>Implement caching strategies</li> <li>Optimize query patterns</li> <li>Monitor memory usage</li> </ul>"},{"location":"usage/memory/#3-data-quality","title":"3. Data Quality","text":"<ul> <li>Validate input data</li> <li>Implement deduplication</li> <li>Regular cleanup and maintenance</li> <li>Backup important memories</li> </ul>"},{"location":"usage/memory/#4-privacy-and-security","title":"4. Privacy and Security","text":"<ul> <li>Implement access controls</li> <li>Encrypt sensitive data</li> <li>Regular security audits</li> <li>Compliance with data regulations</li> </ul>"},{"location":"usage/memory/#monitoring-and-analytics","title":"Monitoring and Analytics","text":"<pre><code>class MemoryAnalytics:\n    def __init__(self, memory_api):\n        self.memory = memory_api\n\n    def get_usage_stats(self, days=30):\n        \"\"\"Get memory usage statistics.\"\"\"\n        from datetime import datetime, timedelta\n\n        cutoff_date = (datetime.now() - timedelta(days=days)).isoformat()\n\n        recent_memories = self.memory.retrieve(\n            query=\"\",\n            top_k=10000,\n            filter={\"timestamp\": {\"$gte\": cutoff_date}}\n        )\n\n        # Analyze usage patterns\n        stats = {\n            \"total_memories\": len(recent_memories),\n            \"by_type\": {},\n            \"by_category\": {},\n            \"storage_growth\": 0\n        }\n\n        for memory in recent_memories:\n            memory_type = memory.metadata.get(\"type\", \"unknown\")\n            stats[\"by_type\"][memory_type] = stats[\"by_type\"].get(memory_type, 0) + 1\n\n            category = memory.metadata.get(\"category\", \"unknown\")\n            stats[\"by_category\"][category] = stats[\"by_category\"].get(category, 0) + 1\n\n        return stats\n</code></pre>"},{"location":"usage/memory/#next-steps","title":"Next Steps","text":"<ul> <li>Vector Stores: Learn about different storage backends</li> <li>Embedding Models: Understand embedding options</li> <li>Examples: See real-world implementations</li> <li>API Reference: Complete API documentation </li> </ul>"},{"location":"usage/vectorstores/","title":"Vector Stores","text":"<p>This page will contain documentation about different vector store backends supported by Memorix AI.</p>"},{"location":"usage/vectorstores/#supported-vector-stores","title":"Supported Vector Stores","text":"<ul> <li>Chroma</li> <li>Pinecone</li> <li>Weaviate</li> <li>Qdrant</li> <li>Milvus</li> </ul> <p>This documentation is under development. </p>"}]}